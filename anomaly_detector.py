# -*- coding: UTF-8 -*-
"""
Anomaly Detector for AMI Smart Grid SDN (Updated v7).

Reads flow statistics from the log file generated by the Ryu controller,
preprocesses the data CONSISTENTLY with the training script (ads.py),
and uses the pre-trained Transformer model to detect network anomalies.
Fixes ValueError during printing of flow details.
"""

import time
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model # type: ignore
import joblib
import warnings
import os
import datetime
from io import StringIO

# --- Configuration ---
FLOW_LOG_FILE = 'flow_stats.log'    # Log file generated by ami_controller.py
MODEL_PATH = 'ads/saved_transformer_model.h5' # Path to your trained model
SCALER_PATH = 'ads/scaler.joblib'
ENCODER_PATH = 'ads/encoder.joblib'
TARGET_ENCODER_PATH = 'ads/target_encoder.joblib'
POLL_INTERVAL = 5 # Reduced interval for potentially faster feedback
LOOKBACK = 1       # Sequence length is 1 based on ads.py's prepare_data_for_transformer

# --- Column Definitions ---
# (Remain the same as v6)
LOG_FILE_COLUMNS = [
    'ts', 'datapath_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'proto',
    'duration', 'src_bytes', 'dst_bytes', 'src_pkts', 'dst_pkts',
    'service', 'conn_state', 'missed_bytes', 'src_ip_bytes', 'dst_ip_bytes',
    'dns_query', 'dns_qclass', 'dns_qtype', 'dns_rcode', 'dns_AA', 'dns_RD', 'dns_RA',
    'dns_rejected', 'ssl_version', 'ssl_cipher', 'ssl_resumed', 'ssl_established',
    'ssl_subject', 'ssl_issuer', 'http_trans_depth', 'http_method', 'http_uri',
    'http_referrer', 'http_version', 'http_request_body_len', 'http_response_body_len',
    'http_status_code', 'http_user_agent', 'http_orig_mime_types', 'http_resp_mime_types',
    'weird_name', 'weird_addl', 'weird_notice'
]
DERIVED_FEATURE_COLS = ['hour', 'day', 'day_of_week', 'src_ip_parsed', 'dst_ip_parsed']
BOOLEAN_COLS = ['dns_AA', 'dns_RD', 'dns_RA', 'dns_rejected', 'ssl_resumed',
                'ssl_established', 'weird_notice']

CATEGORICAL_COLS_FOR_ENCODER = [
    'proto', 'service', 'conn_state', 'dns_query', 'ssl_version',
    'ssl_cipher', 'ssl_subject', 'ssl_issuer', 'http_method',
    'http_uri', 'http_referrer', 'http_version', 'http_user_agent',
    'http_orig_mime_types', 'http_resp_mime_types', 'weird_name',
    'weird_addl'
    ]

NUMERICAL_COLS_FOR_SCALER = [
    'src_port', 'dst_port', 'duration', 'src_bytes', 'dst_bytes', 'missed_bytes',
    'src_pkts', 'src_ip_bytes', 'dst_pkts', 'dst_ip_bytes', 'dns_qclass',
    'dns_qtype', 'dns_rcode',
    'dns_AA', 'dns_RD', 'dns_RA', 'dns_rejected',
    'ssl_resumed', 'ssl_established',
    'http_trans_depth', 'http_request_body_len', 'http_response_body_len',
    'http_status_code',
    'weird_notice',
    'hour', 'day', 'day_of_week', 'src_ip_parsed', 'dst_ip_parsed'
    ]

ANOMALY_MAP = {}

# Suppress TensorFlow warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
tf.get_logger().setLevel('ERROR')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# --- Global Variables ---
model = None
scaler = None
encoder = None
target_encoder = None
processed_rows = 0

# --- Functions (load_resources, preprocess_data, predict_anomalies are same as v6) ---

def load_resources():
    """Loads the trained model and the saved preprocessing objects."""
    global model, scaler, encoder, target_encoder, ANOMALY_MAP
    print("Loading resources...")
    try:
        print(f"Loading model from: {MODEL_PATH}")
        if not os.path.exists(MODEL_PATH): raise FileNotFoundError(f"Model file not found: {MODEL_PATH}")
        model = load_model(MODEL_PATH, compile=False)
        print("Model loaded successfully.")

        print(f"Loading scaler from: {SCALER_PATH}")
        if not os.path.exists(SCALER_PATH): raise FileNotFoundError(f"Scaler file not found: {SCALER_PATH}")
        scaler = joblib.load(SCALER_PATH)
        print("Scaler loaded successfully.")

        print(f"Loading encoder from: {ENCODER_PATH}")
        if not os.path.exists(ENCODER_PATH): raise FileNotFoundError(f"Encoder file not found: {ENCODER_PATH}")
        encoder = joblib.load(ENCODER_PATH)
        print("Encoder loaded successfully.")

        print(f"Loading target encoder from: {TARGET_ENCODER_PATH}")
        if not os.path.exists(TARGET_ENCODER_PATH): raise FileNotFoundError(f"Target encoder file not found: {TARGET_ENCODER_PATH}")
        target_encoder = joblib.load(TARGET_ENCODER_PATH)
        print("Target encoder loaded successfully.")

        if hasattr(target_encoder, 'categories_') and len(target_encoder.categories_) > 0:
            class_names = target_encoder.categories_[0]
            ANOMALY_MAP = {i: name for i, name in enumerate(class_names)}
            print("Automatically generated ANOMALY_MAP from target encoder:")
            print(ANOMALY_MAP)
        else:
            print("ERROR: Could not determine class names from loaded target_encoder.")
            return False
        return True

    except Exception as e:
        print(f"Error loading resources: {e}")
        import traceback
        traceback.print_exc()
        return False

def preprocess_data(df):
    """Preprocesses the dataframe."""
    global scaler, encoder
    # print(f"Preprocessing {len(df)} new rows...")
    try:
        # === 1. Feature Engineering ===
        df['ts_numeric'] = pd.to_numeric(df['ts'], errors='coerce')
        df['datetime'] = pd.to_datetime(df['ts_numeric'], unit='s', errors='coerce')
        df['hour'] = df['datetime'].dt.hour.fillna(0).astype(int)
        df['day'] = df['datetime'].dt.day.fillna(0).astype(int)
        df['day_of_week'] = df['datetime'].dt.dayofweek.fillna(0).astype(int)

        def ip_to_int(ip_str):
            try:
                if isinstance(ip_str, str) and '.' in ip_str:
                    parts = ip_str.strip().strip("'\"").split('.')
                    if len(parts) == 4: return int(''.join([part.zfill(3) for part in parts]))
                return 0
            except: return 0
        df['src_ip_parsed'] = df['src_ip'].apply(ip_to_int)
        df['dst_ip_parsed'] = df['dst_ip'].apply(ip_to_int)

        for col in BOOLEAN_COLS:
            if col in df.columns: df[col] = df[col].apply(lambda x: 1 if str(x).strip().lower() in ['t', 'true', '1'] else 0)
            else: df[col] = 0

        # === 2. Handle Missing Values & Ensure Correct Types ===
        log_categorical_cols = df.select_dtypes(include=['object']).columns
        for col in log_categorical_cols:
             if col in CATEGORICAL_COLS_FOR_ENCODER: df[col] = df[col].fillna('-').astype(str)
             else: df[col] = df[col].fillna('-')

        # print("Applying numeric conversion...")
        for col in NUMERICAL_COLS_FOR_SCALER:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                if not pd.api.types.is_numeric_dtype(df[col]):
                     try: df[col] = df[col].astype(float)
                     except ValueError:
                          print(f"ERROR: Could not force column '{col}' to float. Check data.")
                          return None
            else: df[col] = 0

        # === 3. Select and Order Features ===
        X = pd.DataFrame()
        features_for_model = NUMERICAL_COLS_FOR_SCALER + CATEGORICAL_COLS_FOR_ENCODER
        for col in features_for_model:
            if col in df.columns: X[col] = df[col]
            else: X[col] = '-' if col in CATEGORICAL_COLS_FOR_ENCODER else 0

        X_num_features = X[NUMERICAL_COLS_FOR_SCALER]
        X_cat_features = X[CATEGORICAL_COLS_FOR_ENCODER]

        # === 4. Apply Scaler and Encoder ===
        if scaler is None or encoder is None: return None
        # print("Scaling...")
        X_num_scaled = scaler.transform(X_num_features.astype(float))
        # print("Encoding...")
        X_cat_encoded = encoder.transform(X_cat_features)

        # === 5. Combine and Reshape ===
        X_combined = np.hstack([X_num_scaled, X_cat_encoded])
        X_reshaped = X_combined.reshape(X_combined.shape[0], 1, X_combined.shape[1])
        # print("Preprocessing complete.")
        return X_reshaped

    except Exception as e:
        print(f"Error during preprocessing: {e}")
        # import traceback # Uncomment for detailed trace
        # traceback.print_exc() # Uncomment for detailed trace
        return None


def predict_anomalies(data_sequence):
    """Makes predictions using the loaded model."""
    global model, ANOMALY_MAP
    if model is None or data_sequence.shape[0] == 0: return None
    if not ANOMALY_MAP: return None

    try:
        predictions_proba = model.predict(data_sequence, verbose=0)
        if predictions_proba.ndim != 2:
             print(f"ERROR: Unexpected prediction output dimension: {predictions_proba.ndim}. Expected 2.")
             return None

        results = []
        for i in range(predictions_proba.shape[0]):
             prediction_for_item = predictions_proba[i, :] # Corrected indexing
             predicted_class_index = np.argmax(prediction_for_item)
             confidence = np.max(prediction_for_item)
             anomaly_type = ANOMALY_MAP.get(predicted_class_index, f"Unknown Class ({predicted_class_index})")
             results.append({'type': anomaly_type, 'confidence': confidence})
        return results

    except Exception as e:
        print(f"Error during prediction: {e}")
        import traceback
        traceback.print_exc()
        return None


def monitor_log_file():
    """Monitors the log file for new entries and triggers detection."""
    global processed_rows
    print(f"Monitoring log file: {FLOW_LOG_FILE}")
    while not os.path.exists(FLOW_LOG_FILE):
        print(f"Log file {FLOW_LOG_FILE} not found. Waiting...")
        time.sleep(POLL_INTERVAL)

    try:
        with open(FLOW_LOG_FILE, 'r') as f:
            first_line = f.readline()
            has_header = all(col in first_line for col in ['ts', 'datapath_id', 'src_ip'])
            initial_line_count = sum(1 for _ in f) + (1 if has_header else 0)
            processed_rows = initial_line_count
            print(f"Skipping {processed_rows} existing lines in log file (including header: {has_header}).")
    except Exception as e:
        print(f"Error reading initial log file state: {e}")
        processed_rows = 0

    while True:
        try:
            current_row_count = 0
            new_lines_content = []
            if not os.path.exists(FLOW_LOG_FILE):
                 print(f"Log file {FLOW_LOG_FILE} disappeared. Waiting...")
                 processed_rows = 0
                 time.sleep(POLL_INTERVAL * 2)
                 continue

            with open(FLOW_LOG_FILE, 'r') as f:
                 lines = f.readlines()
                 current_row_count = len(lines)
                 if current_row_count > processed_rows:
                      new_lines_content = lines[processed_rows:]

            if new_lines_content:
                # print(f"Detected {len(new_lines_content)} new log entries.")
                new_data_str = "".join(new_lines_content)
                try:
                    new_df = pd.read_csv(StringIO(new_data_str), names=LOG_FILE_COLUMNS, header=None, dtype=object, on_bad_lines='warn') # Warn about bad lines
                except pd.errors.ParserError as pe:
                     print(f"WARNING: ParserError reading log chunk: {pe}. Skipping this chunk.")
                     processed_rows = current_row_count
                     continue

                original_log_data = new_df.to_dict('records')
                preprocessed_data = preprocess_data(new_df.copy())

                if preprocessed_data is not None and preprocessed_data.shape[0] > 0:
                    predictions = predict_anomalies(preprocessed_data)
                    if predictions:
                        for i, result in enumerate(predictions):
                            anomaly_type = result['type']
                            confidence = result['confidence']
                            is_anomaly = anomaly_type != 'normal'
                            original_entry = original_log_data[i]

                            # Safely get and format timestamp
                            try:
                                ts = float(original_entry.get('ts', time.time()))
                                dt_object = datetime.datetime.fromtimestamp(ts)
                                ts_str = dt_object.strftime('%Y-%m-%d %H:%M:%S')
                            except (ValueError, TypeError):
                                 ts_str = "Invalid Timestamp"

                            # Safely get and format duration
                            try:
                                duration_val = float(original_entry.get('duration', 0))
                                duration_str = f"{duration_val:.4f}s"
                            except (ValueError, TypeError):
                                duration_str = f"{original_entry.get('duration', 'N/A')}" # Print original if not float

                            if is_anomaly:
                                print(f"\n--- ANOMALY DETECTED [{ts_str}] ---")
                                print(f"   Type: {anomaly_type} (Confidence: {confidence:.4f})")
                                print(f"   Flow Details:")
                                print(f"     DPID: {original_entry.get('datapath_id', 'N/A')}")
                                print(f"     SRC: {original_entry.get('src_ip', 'N/A')}:{original_entry.get('src_port', 'N/A')}")
                                print(f"     DST: {original_entry.get('dst_ip', 'N/A')}:{original_entry.get('dst_port', 'N/A')}")
                                # *** MODIFICATION START: Safe duration printing ***
                                print(f"     PROTO: {original_entry.get('proto', 'N/A')}, DURATION: {duration_str}")
                                # *** MODIFICATION END ***
                                print(f"     BYTES(S->D): {original_entry.get('src_bytes', 0)}, PKTS(S->D): {original_entry.get('src_pkts', 0)}")
                                print("--------------------------------------------------")
                            else:
                                print(f"Normal traffic detected [{ts_str}] (Confidence: {confidence:.4f}) SRC: {original_entry.get('src_ip', 'N/A')}:{original_entry.get('src_port', 'N/A')} -> DST: {original_entry.get('dst_ip', 'N/A')}:{original_entry.get('dst_port', 'N/A')}")

                else:
                     print("Preprocessing returned no data or failed. Skipping prediction for this chunk.")

                processed_rows = current_row_count

            else: pass # No new lines

        except pd.errors.EmptyDataError:
             try:
                 with open(FLOW_LOG_FILE, 'r') as f: processed_rows = sum(1 for _ in f)
             except FileNotFoundError: processed_rows = 0
        except Exception as e:
            print(f"An error occurred in monitoring loop: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(POLL_INTERVAL * 2)

        time.sleep(POLL_INTERVAL)


if __name__ == "__main__":
    if load_resources():
        monitor_log_file()
    else:
        print("Failed to load critical resources (Model/Scaler/Encoder). Exiting.")
        print("Please ensure the model (.h5) was saved with the same TensorFlow version being used now.")
        print("Also ensure scaler.joblib, encoder.joblib, and target_encoder.joblib")
        print("are saved correctly from your ads.py training script and placed in the 'ads/' directory.")
