# -*- coding: UTF-8 -*-
"""
Anomaly Detector for AMI Smart Grid SDN (Updated).

Reads flow statistics from the log file generated by the Ryu controller,
preprocesses the data CONSISTENTLY with the training script (ads.py),
and uses the pre-trained Transformer model to detect network anomalies.
"""

import time
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model # type: ignore
# IMPORTANT: You MUST use joblib (or pickle) to save the scaler, encoder,
# and target_encoder from your ads.py script after fitting them on the
# training data. Then load them here.
import joblib
import warnings
import os
import datetime
from io import StringIO

# --- Configuration ---
FLOW_LOG_FILE = 'flow_stats.log'    # Log file generated by ami_controller.py
MODEL_PATH = 'ads/ads/saved_transformer_model.h5' # Path to your trained model
# --- CRITICAL: Paths to load the fitted preprocessing objects ---
# --- You MUST save these in ads.py after fitting ---
SCALER_PATH = 'ads/ads/scaler.joblib'
ENCODER_PATH = 'ads/ads/encoder.joblib'
TARGET_ENCODER_PATH = 'ads/ads/target_encoder.joblib'
# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---

POLL_INTERVAL = 10 # Seconds to wait before checking for new log entries
LOOKBACK = 1       # Sequence length is 1 based on ads.py's prepare_data_for_transformer

# --- Column Definitions ---
# Columns expected in the flow_stats.log file from ami_controller.py
LOG_FILE_COLUMNS = [
    'ts', 'datapath_id', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'proto',
    'duration', 'src_bytes', 'dst_bytes', 'src_pkts', 'dst_pkts',
    'service', 'conn_state', 'missed_bytes', 'src_ip_bytes', 'dst_ip_bytes',
    'dns_query', 'dns_qclass', 'dns_qtype', 'dns_rcode', 'dns_AA', 'dns_RD', 'dns_RA',
    'dns_rejected', 'ssl_version', 'ssl_cipher', 'ssl_resumed', 'ssl_established',
    'ssl_subject', 'ssl_issuer', 'http_trans_depth', 'http_method', 'http_uri',
    'http_referrer', 'http_version', 'http_request_body_len', 'http_response_body_len',
    'http_status_code', 'http_user_agent', 'http_orig_mime_types', 'http_resp_mime_types',
    'weird_name', 'weird_addl', 'weird_notice'
]

# Columns used for FEATURE ENGINEERING (derived from log columns)
DERIVED_FEATURE_COLS = ['hour', 'day', 'day_of_week', 'src_ip_parsed', 'dst_ip_parsed']

# Boolean-like columns to convert to 0/1 (based on ads.py)
BOOLEAN_COLS = ['dns_AA', 'dns_RD', 'dns_RA', 'dns_rejected', 'ssl_resumed',
                'ssl_established', 'weird_notice']

# Categorical columns identified in ads.py (used by the OneHotEncoder)
# Note: This list comes from X.select_dtypes(include=['object']).columns in ads.py
#       after feature engineering and dropping initial columns. Adjust if needed.
#       It MUST match the columns the loaded 'encoder.joblib' expects.
CATEGORICAL_COLS_FOR_ENCODER = [
    'proto', 'service', 'conn_state', 'dns_query', 'ssl_version',
    'ssl_cipher', 'ssl_subject', 'ssl_issuer', 'http_method',
    'http_uri', 'http_referrer', 'http_version', 'http_user_agent',
    'http_orig_mime_types', 'http_resp_mime_types', 'weird_name',
    'weird_addl'
    # Add any other object columns that were present in X_train before encoding in ads.py
    ]

# Numerical columns identified in ads.py (used by the StandardScaler)
# Note: This list comes from X.select_dtypes(include=['int64', 'float64']).columns in ads.py
#       after feature engineering and dropping initial columns. Adjust if needed.
#       It MUST match the columns the loaded 'scaler.joblib' expects.
NUMERICAL_COLS_FOR_SCALER = [
    'src_port', 'dst_port', 'duration', 'src_bytes', 'dst_bytes',
    'missed_bytes', 'src_pkts', 'src_ip_bytes', 'dst_pkts', 'dst_ip_bytes',
    'dns_qclass', 'dns_qtype', 'dns_rcode', 'http_trans_depth',
    'http_request_body_len', 'http_response_body_len', 'http_status_code',
    # Added derived numerical features
    'hour', 'day', 'day_of_week', 'src_ip_parsed', 'dst_ip_parsed',
    # Added converted boolean features
    'dns_AA', 'dns_RD', 'dns_RA', 'dns_rejected', 'ssl_resumed',
    'ssl_established', 'weird_notice'
    # Add any other numerical columns that were present in X_train before scaling in ads.py
    ]

# --- Anomaly Mapping (CRITICAL: Must match training output) ---
# This mapping needs to be derived from the `target_encoder` saved from ads.py
# Example: target_encoder.categories_[0] will give the class names in order.
# You MUST update this based on your actual trained target_encoder.
ANOMALY_MAP = {
    # 0: 'backdoor', # Example - Replace with your actual mapping
    # 1: 'ddos',
    # 2: 'dos',
    # 3: 'injection',
    # 4: 'normal',
    # 5: 'password',
    # 6: 'ransomware',
    # 7: 'scanning',
    # 8: 'xss',
    # ... add all classes in the order determined by target_encoder.categories_[0]
}

# Suppress TensorFlow warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning) # Ignore joblib warnings if needed
tf.get_logger().setLevel('ERROR') # Suppress TensorFlow INFO messages
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress more detailed TF messages

# --- Global Variables ---
model = None
scaler = None # Will be loaded from SCALER_PATH
encoder = None # Will be loaded from ENCODER_PATH
target_encoder = None # Will be loaded from TARGET_ENCODER_PATH
processed_rows = 0 # Keep track of processed log lines

def load_resources():
    """Loads the trained model and the saved preprocessing objects."""
    global model, scaler, encoder, target_encoder, ANOMALY_MAP
    print("Loading resources...")
    try:
        # Load Model
        print(f"Loading model from: {MODEL_PATH}")
        if not os.path.exists(MODEL_PATH):
            print(f"ERROR: Model file not found at {MODEL_PATH}")
            return False
        model = load_model(MODEL_PATH)
        print("Model loaded successfully.")
        # model.summary() # Optional: Print model summary

        # Load Scaler
        print(f"Loading scaler from: {SCALER_PATH}")
        if not os.path.exists(SCALER_PATH):
            print(f"ERROR: Scaler file not found at {SCALER_PATH}")
            print("       You MUST save the scaler object in ads.py after fitting it.")
            return False
        scaler = joblib.load(SCALER_PATH)
        print("Scaler loaded successfully.")

        # Load Encoder
        print(f"Loading encoder from: {ENCODER_PATH}")
        if not os.path.exists(ENCODER_PATH):
            print(f"ERROR: Encoder file not found at {ENCODER_PATH}")
            print("       You MUST save the encoder object in ads.py after fitting it.")
            return False
        encoder = joblib.load(ENCODER_PATH)
        print("Encoder loaded successfully.")

        # Load Target Encoder (to get class mapping)
        print(f"Loading target encoder from: {TARGET_ENCODER_PATH}")
        if not os.path.exists(TARGET_ENCODER_PATH):
            print(f"ERROR: Target encoder file not found at {TARGET_ENCODER_PATH}")
            print("       You MUST save the target_encoder object in ads.py after fitting it.")
            return False
        target_encoder = joblib.load(TARGET_ENCODER_PATH)
        print("Target encoder loaded successfully.")

        # --- Automatically generate ANOMALY_MAP from the loaded target_encoder ---
        if hasattr(target_encoder, 'categories_') and len(target_encoder.categories_) > 0:
            class_names = target_encoder.categories_[0]
            ANOMALY_MAP = {i: name for i, name in enumerate(class_names)}
            print("Automatically generated ANOMALY_MAP from target encoder:")
            print(ANOMALY_MAP)
        else:
            print("ERROR: Could not determine class names from loaded target_encoder.")
            print("       Please ensure it's the correct OneHotEncoder object and was fitted.")
            print("       You may need to manually define ANOMALY_MAP.")
            # Keep the potentially manually defined map, but warn the user.
            if not ANOMALY_MAP: # If it wasn't manually defined either
                 return False

        return True

    except Exception as e:
        print(f"Error loading resources: {e}")
        import traceback
        traceback.print_exc()
        return False

def preprocess_data(df):
    """
    Preprocesses the dataframe (from log file) to match the model's input requirements,
    replicating the steps in ads.py and using LOADED scaler/encoders.
    """
    global scaler, encoder
    print(f"Preprocessing {len(df)} new rows...")

    # === 1. Replicate Feature Engineering from ads.py ===
    # Convert timestamp
    df['ts_numeric'] = pd.to_numeric(df['ts'], errors='coerce')
    df['datetime'] = pd.to_datetime(df['ts_numeric'], unit='s', errors='coerce')
    nat_count = df['datetime'].isna().sum()
    if nat_count > 0:
        print(f"Warning: {nat_count} invalid timestamp values found.")
    df['hour'] = df['datetime'].dt.hour.fillna(0).astype(int)
    df['day'] = df['datetime'].dt.day.fillna(0).astype(int)
    df['day_of_week'] = df['datetime'].dt.dayofweek.fillna(0).astype(int)

    # Convert IPs
    def ip_to_int(ip_str):
        try:
            if isinstance(ip_str, str) and '.' in ip_str:
                # Handle potential extra quotes or spaces
                parts = ip_str.strip().strip("'\"").split('.')
                if len(parts) == 4:
                     # Pad with zeros and concatenate
                     return int(''.join([part.zfill(3) for part in parts]))
            return 0 # Return 0 for invalid formats or non-strings
        except:
            return 0 # Catch any conversion errors

    df['src_ip_parsed'] = df['src_ip'].apply(ip_to_int)
    df['dst_ip_parsed'] = df['dst_ip'].apply(ip_to_int)

    # Convert boolean-like columns
    for col in BOOLEAN_COLS:
        if col in df.columns:
            # Handle various representations ('T', 't', True, 1, '1', '-', etc.)
            df[col] = df[col].apply(lambda x: 1 if str(x).strip().lower() in ['t', 'true', '1'] else 0)
        else:
            print(f"Warning: Boolean column '{col}' not found in log data. Adding as 0.")
            df[col] = 0 # Add column if missing

    # === 2. Handle Missing Values (Consistent with ads.py) ===
    # Identify categorical and numerical columns *present in the log data*
    log_categorical_cols = df.select_dtypes(include=['object']).columns
    log_numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

    # Fill missing categorical with '-'
    for col in log_categorical_cols:
         # Check if column should be treated as categorical for encoding later
         if col in CATEGORICAL_COLS_FOR_ENCODER:
              df[col] = df[col].fillna('-').astype(str) # Ensure string type for encoder
         else:
              # Handle other object columns if necessary (e.g., datapath_id)
              df[col] = df[col].fillna('-')


    # Fill missing numerical with 0 (after numeric conversion)
    # Exclude columns we handle specially or will drop
    cols_to_exclude_from_num_fill = ['ts', 'ts_numeric', 'datetime', 'src_ip', 'dst_ip', 'datapath_id']
    for col in log_numerical_cols:
        if col not in cols_to_exclude_from_num_fill:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

    # === 3. Select and Order Features ===
    # Create the feature set 'X' that matches the input to scaling/encoding in ads.py
    # This involves selecting the right columns (original + derived) and dropping others.
    features_for_model = NUMERICAL_COLS_FOR_SCALER + CATEGORICAL_COLS_FOR_ENCODER
    X = pd.DataFrame()
    missing_cols_in_log = []
    for col in features_for_model:
        if col in df.columns:
            X[col] = df[col]
        else:
            # This should ideally not happen if LOG_FILE_COLUMNS and derived features are correct
            print(f"CRITICAL WARNING: Column '{col}' expected for model input but not found in processed log data. Filling with 0 or '-'. Check column definitions.")
            missing_cols_in_log.append(col)
            # Decide fill value based on presumed type (crude guess)
            if col in CATEGORICAL_COLS_FOR_ENCODER:
                 X[col] = '-'
            else:
                 X[col] = 0

    if missing_cols_in_log:
         print(f"Missing columns filled: {missing_cols_in_log}")

    # Ensure column order matches exactly what the scaler/encoder expect
    # (This is implicitly handled if NUMERICAL_COLS_FOR_SCALER and CATEGORICAL_COLS_FOR_ENCODER
    # lists are correct and ordered as they were during training)
    X_num_features = X[NUMERICAL_COLS_FOR_SCALER]
    X_cat_features = X[CATEGORICAL_COLS_FOR_ENCODER]


    # === 4. Apply PRE-FITTED Scaler and Encoder ===
    if scaler is None or encoder is None:
        print("ERROR: Scaler or Encoder not loaded. Cannot preprocess.")
        return None

    try:
        # Scale Numerical Features
        print(f"Scaling {X_num_features.shape[1]} numerical features...")
        X_num_scaled = scaler.transform(X_num_features)

        # Encode Categorical Features
        print(f"Encoding {X_cat_features.shape[1]} categorical features...")
        X_cat_encoded = encoder.transform(X_cat_features)

    except ValueError as e:
         print(f"ERROR during scaling/encoding: {e}")
         print("This often means the columns in the live data (after feature engineering)")
         print("do not EXACTLY match the columns the scaler/encoder were trained on.")
         print("Check NUMERICAL_COLS_FOR_SCALER and CATEGORICAL_COLS_FOR_ENCODER lists.")
         print("Input numerical columns:", X_num_features.columns.tolist())
         print("Input categorical columns:", X_cat_features.columns.tolist())
         if hasattr(scaler, 'feature_names_in_'): print("Scaler expected:", scaler.feature_names_in_)
         if hasattr(encoder, 'feature_names_in_'): print("Encoder expected:", encoder.feature_names_in_)
         return None
    except Exception as e:
         print(f"Unexpected error during scaling/encoding: {e}")
         import traceback
         traceback.print_exc()
         return None


    # === 5. Combine and Reshape for Transformer (LOOKBACK=1) ===
    X_combined = np.hstack([X_num_scaled, X_cat_encoded])
    # Reshape to (batch_size, 1, num_features)
    X_reshaped = X_combined.reshape(X_combined.shape[0], 1, X_combined.shape[1])

    print(f"Preprocessing complete. Output shape for model: {X_reshaped.shape}")
    return X_reshaped


def predict_anomalies(data_sequence):
    """Makes predictions using the loaded model."""
    global model, ANOMALY_MAP
    if model is None or data_sequence.shape[0] == 0:
        print("Model not loaded or no data sequence provided.")
        return None
    if not ANOMALY_MAP:
        print("ERROR: ANOMALY_MAP is not defined. Cannot map predictions.")
        return None

    # print(f"Predicting on sequence of shape: {data_sequence.shape}") # Verbose
    try:
        predictions_proba = model.predict(data_sequence) # Shape: (batch, 1, num_classes)

        # Process predictions for each item in the batch
        results = []
        for i in range(predictions_proba.shape[0]):
             # Get prediction for the single timestep (index 0)
             prediction_for_item = predictions_proba[i, 0, :] # Shape: (num_classes,)
             predicted_class_index = np.argmax(prediction_for_item)
             confidence = np.max(prediction_for_item)
             anomaly_type = ANOMALY_MAP.get(predicted_class_index, f"Unknown Class ({predicted_class_index})")
             results.append({'type': anomaly_type, 'confidence': confidence})

        return results # Return list of prediction dicts

    except Exception as e:
        print(f"Error during prediction: {e}")
        import traceback
        traceback.print_exc()
        return None


def monitor_log_file():
    """Monitors the log file for new entries and triggers detection."""
    global processed_rows
    print(f"Monitoring log file: {FLOW_LOG_FILE}")
    # Ensure log file exists before starting
    while not os.path.exists(FLOW_LOG_FILE):
        print(f"Log file {FLOW_LOG_FILE} not found. Waiting...")
        time.sleep(POLL_INTERVAL)

    # Read initial state (or skip existing lines)
    try:
        # Count initial lines to skip them on first read
        with open(FLOW_LOG_FILE, 'r') as f:
            # Skip header if present
            first_line = f.readline()
            has_header = all(col in first_line for col in ['ts', 'datapath_id', 'src_ip']) # Simple check
            initial_line_count = sum(1 for _ in f) + (1 if has_header else 0) # Add header back if present
            processed_rows = initial_line_count
            print(f"Skipping {processed_rows} existing lines in log file (including header: {has_header}).")
    except Exception as e:
        print(f"Error reading initial log file state: {e}")
        processed_rows = 0 # Start from beginning if error

    while True:
        try:
            current_row_count = 0
            new_lines_content = []
            with open(FLOW_LOG_FILE, 'r') as f:
                 lines = f.readlines()
                 current_row_count = len(lines)
                 if current_row_count > processed_rows:
                      new_lines_content = lines[processed_rows:]

            if new_lines_content:
                print(f"Detected {len(new_lines_content)} new log entries.")
                # Use StringIO to read CSV data from the list of strings
                # Pass column names explicitly as the header might not be in the new chunk
                new_data_str = "".join(new_lines_content)
                new_df = pd.read_csv(StringIO(new_data_str), names=LOG_FILE_COLUMNS, header=None)

                # --- Store original log data before preprocessing for context ---
                original_log_data = new_df.to_dict('records')

                # Preprocess the new data
                preprocessed_data = preprocess_data(new_df.copy()) # Use copy

                if preprocessed_data is not None and preprocessed_data.shape[0] > 0:
                    # Predict (batch prediction)
                    predictions = predict_anomalies(preprocessed_data)

                    if predictions:
                        # Process results for each predicted entry
                        for i, result in enumerate(predictions):
                            anomaly_type = result['type']
                            confidence = result['confidence']

                            # --- Output Detection Result ---
                            # Only print if it's not 'normal' or meets a threshold (optional)
                            is_anomaly = anomaly_type != 'normal' # Adjust if 'normal' class name differs

                            if is_anomaly: # Or add: and confidence > 0.7:
                                original_entry = original_log_data[i]
                                dt_object = datetime.datetime.fromtimestamp(original_entry['ts'])
                                print(f"\n--- ANOMALY DETECTED [{dt_object.strftime('%Y-%m-%d %H:%M:%S')}] ---")
                                print(f"   Type: {anomaly_type} (Confidence: {confidence:.4f})")
                                print(f"   Flow Details:")
                                print(f"     DPID: {original_entry.get('datapath_id', 'N/A')}")
                                print(f"     SRC: {original_entry.get('src_ip', 'N/A')}:{original_entry.get('src_port', 'N/A')}")
                                print(f"     DST: {original_entry.get('dst_ip', 'N/A')}:{original_entry.get('dst_port', 'N/A')}")
                                print(f"     PROTO: {original_entry.get('proto', 'N/A')}, DURATION: {original_entry.get('duration', 0):.4f}s")
                                print(f"     BYTES(S->D): {original_entry.get('src_bytes', 0)}, PKTS(S->D): {original_entry.get('src_pkts', 0)}")
                                print("--------------------------------------------------")

                                # --- TODO: Implement Mitigation Action ---
                                # e.g., send command to Ryu controller via REST API
                                # block_malicious_flow(original_entry['src_ip'], ...)
                                # ------------------------------------------

                # Update the count of processed rows AFTER successful processing
                processed_rows = current_row_count

            else:
                # print("No new log entries.") # Optional: reduce verbosity
                pass

        except FileNotFoundError:
            print(f"Log file {FLOW_LOG_FILE} disappeared. Waiting...")
            processed_rows = 0 # Reset if file is gone
            time.sleep(POLL_INTERVAL * 2) # Wait longer
        except pd.errors.EmptyDataError:
             print("Log file is empty or new lines are empty. Waiting...")
             # Update row count if file was truncated/emptied
             try:
                 with open(FLOW_LOG_FILE, 'r') as f:
                     processed_rows = sum(1 for _ in f)
             except FileNotFoundError:
                  processed_rows = 0
        except Exception as e:
            print(f"An error occurred in monitoring loop: {e}")
            import traceback
            traceback.print_exc()
            # Optional: Add a longer sleep after an error
            time.sleep(POLL_INTERVAL * 2)

        # Wait before checking again
        time.sleep(POLL_INTERVAL)


if __name__ == "__main__":
    if load_resources():
        monitor_log_file()
    else:
        print("Failed to load critical resources (Model/Scaler/Encoder). Exiting.")
        print("Please ensure scaler.joblib, encoder.joblib, and target_encoder.joblib")
        print("are saved correctly from your ads.py training script and placed in the 'ads/' directory.")

